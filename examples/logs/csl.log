Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Mixed precision type: no

Sample 3648 of the training set: {'input_ids': [101, 3308, 2750, 118, 1264, 1715, 7678, 2853, 727, 7678, 367, 4989, 1264, 1715, 102, 914, 4329, 2736, 1278, 133, 3308, 2750, 118, 1264, 1715, 202, 2853, 727, 135, 225, 117, 668, 2941, 270, 1278, 320, 367, 4989, 1264, 1715, 691, 4987, 1746, 233, 242, 1264, 1715, 607, 691, 3308, 2750, 118, 1264, 1715, 4334, 197, 4635, 1264, 1715, 394, 4896, 6434, 879, 4334, 6498, 4657, 225, 117, 2853, 727, 336, 219, 4635, 2079, 2047, 1657, 1278, 1977, 2807, 4045, 659, 201, 917, 4334, 873, 1800, 2158, 749, 119, 6507, 219, 4635, 2853, 727, 2079, 2047, 668, 687, 2798, 131, 394, 4045, 232, 1264, 1715, 1746, 3245, 5974, 727, 6741, 6501, 4179, 4334, 2807, 2179, 2853, 727, 178, 1264, 1715, 6548, 6498, 5931, 2689, 727, 6741, 1699, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 1}.
Sample 819 of the training set: {'input_ids': [101, 6148, 6114, 7678, 1785, 2179, 1237, 5225, 7678, 773, 1301, 102, 122, 4258, 427, 4788, 318, 2232, 5152, 117, 1501, 2179, 117, 7713, 1816, 117, 5307, 3107, 4977, 3023, 4258, 3766, 690, 7065, 2897, 919, 5310, 6649, 6542, 3582, 7078, 6322, 178, 5310, 4268, 122, 223, 2877, 119, 2756, 2094, 2874, 6114, 2735, 833, 1916, 1237, 5225, 119, 1512, 4636, 3079, 2982, 131, 5310, 6649, 6046, 868, 773, 1301, 117, 919, 4685, 4713, 6046, 4268, 117, 1650, 1709, 6046, 6114, 201, 3574, 117, 5150, 5757, 232, 833, 1916, 1237, 5225, 5637, 2400, 6434, 119, 143, 6325, 3079, 2982, 131, 5310, 5296, 601, 1237, 2179, 5225, 1301, 117, 198, 5042, 6492, 709, 4688, 199, 117, 199, 5042, 1975, 5172, 7380, 3330, 1975, 117, 2876, 1465, 707, 919, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
Sample 9012 of the training set: {'input_ids': [101, 1960, 6028, 7678, 812, 298, 7678, 668, 2941, 102, 686, 4179, 1960, 6028, 6027, 3485, 6283, 2723, 994, 8072, 10775, 607, 668, 2941, 6283, 2723, 117, 4977, 913, 1461, 2829, 757, 7124, 6492, 4762, 6283, 2723, 117, 1746, 9417, 1976, 123, 2877, 130, 118, 7692, 2757, 3352, 5447, 1465, 6649, 668, 1285, 792, 659, 4045, 4334, 219, 3222, 2064, 7056, 7119, 6498, 4657, 6509, 5931, 270, 2350, 1246, 668, 2941, 119, 4977, 2945, 5942, 2782, 131, 130, 2757, 4334, 7056, 7119, 2798, 4186, 225, 1788, 6017, 7252, 3182, 214, 4652, 2088, 1037, 117, 3182, 707, 6017, 812, 2845, 3624, 3310, 3476, 1278, 390, 1788, 628, 4686, 3310, 1344, 198, 3896, 799, 2375, 5344, 117, 2798, 192, 3222, 1788, 3948, 279, 7056, 7119, 132, 7692, 2757, 4334, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 0}.
***** Running training *****
  Num examples = 20000
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 6250
completed_steps 50 - loss: 0.69650394
completed_steps 100 - loss: 0.62747961
completed_steps 150 - loss: 0.43004331
completed_steps 200 - loss: 0.45940199
completed_steps 250 - loss: 0.32274437
completed_steps 300 - loss: 0.51573020
completed_steps 350 - loss: 0.38008544
completed_steps 400 - loss: 0.33523867
completed_steps 450 - loss: 0.45055103
completed_steps 500 - loss: 0.48057356
completed_steps 550 - loss: 0.29437247
completed_steps 600 - loss: 0.37070668
epoch 0: {'accuracy': 0.832}
completed_steps 650 - loss: 0.36316383
completed_steps 700 - loss: 0.32861584
completed_steps 750 - loss: 0.30463159
completed_steps 800 - loss: 0.21419363
completed_steps 850 - loss: 0.41903892
completed_steps 900 - loss: 0.10216647
completed_steps 950 - loss: 0.35701367
completed_steps 1000 - loss: 0.25210506
completed_steps 1050 - loss: 0.44687745
completed_steps 1100 - loss: 0.07619800
completed_steps 1150 - loss: 0.31330687
completed_steps 1200 - loss: 0.27162966
completed_steps 1250 - loss: 0.39918837
epoch 1: {'accuracy': 0.855}
completed_steps 1300 - loss: 0.30697247
completed_steps 1350 - loss: 0.43299806
completed_steps 1400 - loss: 0.14689858
completed_steps 1450 - loss: 0.19426750
completed_steps 1500 - loss: 0.35675594
completed_steps 1550 - loss: 0.26794896
completed_steps 1600 - loss: 0.05203710
completed_steps 1650 - loss: 0.19672665
completed_steps 1700 - loss: 0.18877254
completed_steps 1750 - loss: 0.23515379
completed_steps 1800 - loss: 0.27741280
completed_steps 1850 - loss: 0.16263072
epoch 2: {'accuracy': 0.8513333333333334}
completed_steps 1900 - loss: 0.15958820
completed_steps 1950 - loss: 0.11135828
completed_steps 2000 - loss: 0.14663270
completed_steps 2050 - loss: 0.12805273
completed_steps 2100 - loss: 0.16253462
completed_steps 2150 - loss: 0.18715638
completed_steps 2200 - loss: 0.26059011
completed_steps 2250 - loss: 0.14784555
completed_steps 2300 - loss: 0.36800119
completed_steps 2350 - loss: 0.36325896
completed_steps 2400 - loss: 0.12587082
completed_steps 2450 - loss: 0.47889969
completed_steps 2500 - loss: 0.17306061
epoch 3: {'accuracy': 0.851}
completed_steps 2550 - loss: 0.07447342
completed_steps 2600 - loss: 0.12954268
completed_steps 2650 - loss: 0.19894964
completed_steps 2700 - loss: 0.03815815
completed_steps 2750 - loss: 0.09975138
completed_steps 2800 - loss: 0.11352109
completed_steps 2850 - loss: 0.12370608
completed_steps 2900 - loss: 0.12869152
completed_steps 2950 - loss: 0.27353287
completed_steps 3000 - loss: 0.27919719
completed_steps 3050 - loss: 0.12148043
completed_steps 3100 - loss: 0.19615406
epoch 4: {'accuracy': 0.8496666666666667}
completed_steps 3150 - loss: 0.12851419
completed_steps 3200 - loss: 0.19653928
completed_steps 3250 - loss: 0.12769587
completed_steps 3300 - loss: 0.09911212
completed_steps 3350 - loss: 0.39644554
completed_steps 3400 - loss: 0.05658821
completed_steps 3450 - loss: 0.01504540
completed_steps 3500 - loss: 0.27528939
completed_steps 3550 - loss: 0.10658937
completed_steps 3600 - loss: 0.13994487
completed_steps 3650 - loss: 0.07239290
completed_steps 3700 - loss: 0.07121530
completed_steps 3750 - loss: 0.12464922
epoch 5: {'accuracy': 0.842}
completed_steps 3800 - loss: 0.01252644
completed_steps 3850 - loss: 0.19551288
completed_steps 3900 - loss: 0.07467335
completed_steps 3950 - loss: 0.01576577
completed_steps 4000 - loss: 0.16664377
completed_steps 4050 - loss: 0.08733328
completed_steps 4100 - loss: 0.03845289
completed_steps 4150 - loss: 0.04017934
completed_steps 4200 - loss: 0.11773560
completed_steps 4250 - loss: 0.01566182
completed_steps 4300 - loss: 0.02981949
completed_steps 4350 - loss: 0.09794129
epoch 6: {'accuracy': 0.8426666666666667}
completed_steps 4400 - loss: 0.08465752
completed_steps 4450 - loss: 0.02881412
completed_steps 4500 - loss: 0.02127481
completed_steps 4550 - loss: 0.05310939
completed_steps 4600 - loss: 0.03667954
completed_steps 4650 - loss: 0.03070615
completed_steps 4700 - loss: 0.01341235
completed_steps 4750 - loss: 0.07044231
completed_steps 4800 - loss: 0.01311762
completed_steps 4850 - loss: 0.04435467
completed_steps 4900 - loss: 0.01607751
completed_steps 4950 - loss: 0.11584860
completed_steps 5000 - loss: 0.12955736
epoch 7: {'accuracy': 0.825}
completed_steps 5050 - loss: 0.05156968
completed_steps 5100 - loss: 0.00732800
completed_steps 5150 - loss: 0.20382795
completed_steps 5200 - loss: 0.01063361
completed_steps 5250 - loss: 0.06689866
completed_steps 5300 - loss: 0.29118130
completed_steps 5350 - loss: 0.02421117
completed_steps 5400 - loss: 0.01099375
completed_steps 5450 - loss: 0.00353079
completed_steps 5500 - loss: 0.00734237
completed_steps 5550 - loss: 0.09044939
completed_steps 5600 - loss: 0.07326004
epoch 8: {'accuracy': 0.8386666666666667}
completed_steps 5650 - loss: 0.00968994
completed_steps 5700 - loss: 0.08850870
completed_steps 5750 - loss: 0.00130861
completed_steps 5800 - loss: 0.00520886
completed_steps 5850 - loss: 0.04421342
completed_steps 5900 - loss: 0.07402299
completed_steps 5950 - loss: 0.04283362
completed_steps 6000 - loss: 0.02389249
completed_steps 6050 - loss: 0.02953627
completed_steps 6100 - loss: 0.03834766
completed_steps 6150 - loss: 0.00122014
completed_steps 6200 - loss: 0.00884456
completed_steps 6250 - loss: 0.00762635
epoch 9: {'accuracy': 0.8353333333333334}
