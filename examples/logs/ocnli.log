Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Mixed precision type: no

Sample 41905 of the training set: {'input_ids': [101, 199, 3506, 192, 4935, 1917, 6283, 117, 687, 306, 192, 223, 2877, 1761, 7692, 2921, 566, 6810, 117, 896, 2808, 1715, 3284, 2877, 1776, 6018, 1761, 8268, 1461, 566, 102, 2808, 1715, 852, 2878, 1917, 6283, 6018, 3289, 593, 326, 306, 390, 179, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 2}.
Sample 7296 of the training set: {'input_ids': [101, 3567, 582, 2037, 1800, 7264, 1028, 5537, 1028, 1685, 583, 206, 7209, 2709, 3403, 117, 520, 583, 1977, 220, 3026, 2395, 5931, 294, 1028, 6255, 6741, 1685, 583, 2998, 635, 102, 201, 2709, 3403, 7264, 1028, 5537, 1028, 1685, 583, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 2}.
Sample 1639 of the training set: {'input_ids': [101, 1459, 7163, 4334, 209, 4200, 4967, 1976, 1278, 6509, 5931, 6040, 727, 383, 4334, 117, 6252, 199, 7041, 2921, 4334, 306, 117, 449, 2921, 691, 6612, 3052, 6507, 3022, 4334, 1285, 2739, 102, 4573, 362, 4720, 272, 4334, 1473, 6252, 5152, 362, 4626, 2037, 6612, 3052, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 2}.
***** Running training *****
  Num examples = 50437
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 15770
completed_steps 50 - loss: 1.08218265
completed_steps 100 - loss: 1.13857996
completed_steps 150 - loss: 1.05447853
completed_steps 200 - loss: 1.02720463
completed_steps 250 - loss: 0.97316080
completed_steps 300 - loss: 0.63704425
completed_steps 350 - loss: 0.81136817
completed_steps 400 - loss: 0.71161091
completed_steps 450 - loss: 0.61166859
completed_steps 500 - loss: 0.91144758
completed_steps 550 - loss: 0.55880392
completed_steps 600 - loss: 0.73283929
completed_steps 650 - loss: 0.47136554
completed_steps 700 - loss: 0.85466129
completed_steps 750 - loss: 0.71880656
completed_steps 800 - loss: 0.92177314
completed_steps 850 - loss: 0.70476234
completed_steps 900 - loss: 0.60354418
completed_steps 950 - loss: 0.55951738
completed_steps 1000 - loss: 0.77311462
completed_steps 1050 - loss: 0.38620645
completed_steps 1100 - loss: 0.54673588
completed_steps 1150 - loss: 0.66018808
completed_steps 1200 - loss: 0.65033239
completed_steps 1250 - loss: 0.59054345
completed_steps 1300 - loss: 0.61085290
completed_steps 1350 - loss: 0.95492947
completed_steps 1400 - loss: 0.67058307
completed_steps 1450 - loss: 0.61166567
completed_steps 1500 - loss: 0.84582442
completed_steps 1550 - loss: 0.77675366
epoch 0: {'accuracy': 0.7440677966101695}
completed_steps 1600 - loss: 0.51475143
completed_steps 1650 - loss: 0.60155845
completed_steps 1700 - loss: 0.36524427
completed_steps 1750 - loss: 0.51869476
completed_steps 1800 - loss: 0.46502864
completed_steps 1850 - loss: 0.32843244
completed_steps 1900 - loss: 0.51424205
completed_steps 1950 - loss: 0.53094184
completed_steps 2000 - loss: 0.47768164
completed_steps 2050 - loss: 0.55777448
completed_steps 2100 - loss: 0.45000488
completed_steps 2150 - loss: 0.47528002
completed_steps 2200 - loss: 0.38649976
completed_steps 2250 - loss: 0.52694887
completed_steps 2300 - loss: 0.55363822
completed_steps 2350 - loss: 0.53336370
completed_steps 2400 - loss: 0.44572297
completed_steps 2450 - loss: 0.51303834
completed_steps 2500 - loss: 0.50778013
completed_steps 2550 - loss: 0.40621313
completed_steps 2600 - loss: 0.76125193
completed_steps 2650 - loss: 0.56998235
completed_steps 2700 - loss: 0.63873798
completed_steps 2750 - loss: 0.32355431
completed_steps 2800 - loss: 0.51085049
completed_steps 2850 - loss: 0.54384941
completed_steps 2900 - loss: 0.53997499
completed_steps 2950 - loss: 0.63549864
completed_steps 3000 - loss: 0.55291843
completed_steps 3050 - loss: 0.79065287
completed_steps 3100 - loss: 0.71491921
completed_steps 3150 - loss: 0.49607474
epoch 1: {'accuracy': 0.7569491525423728}
completed_steps 3200 - loss: 0.61515713
completed_steps 3250 - loss: 0.17876977
completed_steps 3300 - loss: 0.23437236
completed_steps 3350 - loss: 0.32646433
completed_steps 3400 - loss: 0.34772980
completed_steps 3450 - loss: 0.19284944
completed_steps 3500 - loss: 0.40536109
completed_steps 3550 - loss: 0.51420861
completed_steps 3600 - loss: 0.21847862
completed_steps 3650 - loss: 0.28003788
completed_steps 3700 - loss: 0.62697417
completed_steps 3750 - loss: 0.53806245
completed_steps 3800 - loss: 0.35396421
completed_steps 3850 - loss: 0.21972628
completed_steps 3900 - loss: 0.48203352
completed_steps 3950 - loss: 0.38283226
completed_steps 4000 - loss: 0.32188299
completed_steps 4050 - loss: 0.55750269
completed_steps 4100 - loss: 0.33865613
completed_steps 4150 - loss: 0.29016373
completed_steps 4200 - loss: 0.35401839
completed_steps 4250 - loss: 0.33219659
completed_steps 4300 - loss: 0.52186364
completed_steps 4350 - loss: 0.30883601
completed_steps 4400 - loss: 0.51636624
completed_steps 4450 - loss: 0.21978141
completed_steps 4500 - loss: 0.17249261
completed_steps 4550 - loss: 0.44192240
completed_steps 4600 - loss: 0.31453106
completed_steps 4650 - loss: 0.14885378
completed_steps 4700 - loss: 0.33624923
epoch 2: {'accuracy': 0.7450847457627119}
completed_steps 4750 - loss: 0.17923978
completed_steps 4800 - loss: 0.17721285
completed_steps 4850 - loss: 0.24561730
completed_steps 4900 - loss: 0.20104462
completed_steps 4950 - loss: 0.12351035
completed_steps 5000 - loss: 0.16918032
completed_steps 5050 - loss: 0.30099264
completed_steps 5100 - loss: 0.12608615
completed_steps 5150 - loss: 0.15977858
completed_steps 5200 - loss: 0.06601199
completed_steps 5250 - loss: 0.17527954
completed_steps 5300 - loss: 0.43403551
completed_steps 5350 - loss: 0.06330729
completed_steps 5400 - loss: 0.33115932
completed_steps 5450 - loss: 0.15243024
completed_steps 5500 - loss: 0.06153747
completed_steps 5550 - loss: 0.17211902
completed_steps 5600 - loss: 0.23031047
completed_steps 5650 - loss: 0.10947905
completed_steps 5700 - loss: 0.32708108
completed_steps 5750 - loss: 0.24822211
completed_steps 5800 - loss: 0.13885388
completed_steps 5850 - loss: 0.12033729
completed_steps 5900 - loss: 0.17924909
completed_steps 5950 - loss: 0.24423084
completed_steps 6000 - loss: 0.30665419
completed_steps 6050 - loss: 0.40871093
completed_steps 6100 - loss: 0.14848885
completed_steps 6150 - loss: 0.15042117
completed_steps 6200 - loss: 0.23503870
completed_steps 6250 - loss: 0.23155008
completed_steps 6300 - loss: 0.20361215
epoch 3: {'accuracy': 0.7538983050847458}
completed_steps 6350 - loss: 0.06506552
completed_steps 6400 - loss: 0.05188461
completed_steps 6450 - loss: 0.12329053
completed_steps 6500 - loss: 0.03360876
completed_steps 6550 - loss: 0.20680782
completed_steps 6600 - loss: 0.05687403
completed_steps 6650 - loss: 0.11089592
completed_steps 6700 - loss: 0.18534896
completed_steps 6750 - loss: 0.06608123
completed_steps 6800 - loss: 0.14587033
completed_steps 6850 - loss: 0.10089564
completed_steps 6900 - loss: 0.10189856
completed_steps 6950 - loss: 0.01979209
completed_steps 7000 - loss: 0.07001267
completed_steps 7050 - loss: 0.23769380
completed_steps 7100 - loss: 0.16011888
completed_steps 7150 - loss: 0.13720767
completed_steps 7200 - loss: 0.17300078
completed_steps 7250 - loss: 0.22708781
completed_steps 7300 - loss: 0.08927541
completed_steps 7350 - loss: 0.21988283
completed_steps 7400 - loss: 0.10928707
completed_steps 7450 - loss: 0.28416267
completed_steps 7500 - loss: 0.18377078
completed_steps 7550 - loss: 0.16400576
completed_steps 7600 - loss: 0.09309403
completed_steps 7650 - loss: 0.14368927
completed_steps 7700 - loss: 0.12759754
completed_steps 7750 - loss: 0.19096518
completed_steps 7800 - loss: 0.13747163
completed_steps 7850 - loss: 0.16611549
epoch 4: {'accuracy': 0.7383050847457627}
completed_steps 7900 - loss: 0.04777158
completed_steps 7950 - loss: 0.01260314
completed_steps 8000 - loss: 0.02159235
completed_steps 8050 - loss: 0.13949312
completed_steps 8100 - loss: 0.00533873
completed_steps 8150 - loss: 0.03000779
completed_steps 8200 - loss: 0.01295566
completed_steps 8250 - loss: 0.02915071
completed_steps 8300 - loss: 0.11876879
completed_steps 8350 - loss: 0.04968739
completed_steps 8400 - loss: 0.02921974
completed_steps 8450 - loss: 0.07281267
completed_steps 8500 - loss: 0.01282027
completed_steps 8550 - loss: 0.01486360
completed_steps 8600 - loss: 0.17306401
completed_steps 8650 - loss: 0.15888867
completed_steps 8700 - loss: 0.01067703
completed_steps 8750 - loss: 0.11209860
completed_steps 8800 - loss: 0.03848044
completed_steps 8850 - loss: 0.09131674
completed_steps 8900 - loss: 0.11366881
completed_steps 8950 - loss: 0.05776036
completed_steps 9000 - loss: 0.03628281
completed_steps 9050 - loss: 0.09831719
completed_steps 9100 - loss: 0.16963097
completed_steps 9150 - loss: 0.04984814
completed_steps 9200 - loss: 0.07916532
completed_steps 9250 - loss: 0.19752082
completed_steps 9300 - loss: 0.07678672
completed_steps 9350 - loss: 0.06819723
completed_steps 9400 - loss: 0.01872543
completed_steps 9450 - loss: 0.03397369
epoch 5: {'accuracy': 0.7423728813559322}
completed_steps 9500 - loss: 0.12420064
completed_steps 9550 - loss: 0.01607149
completed_steps 9600 - loss: 0.00308851
completed_steps 9650 - loss: 0.06284186
completed_steps 9700 - loss: 0.04575370
completed_steps 9750 - loss: 0.01519843
completed_steps 9800 - loss: 0.00663463
completed_steps 9850 - loss: 0.01825575
completed_steps 9900 - loss: 0.00814255
completed_steps 9950 - loss: 0.00990065
completed_steps 10000 - loss: 0.03128296
completed_steps 10050 - loss: 0.08865664
completed_steps 10100 - loss: 0.07287590
completed_steps 10150 - loss: 0.00196530
completed_steps 10200 - loss: 0.02092542
completed_steps 10250 - loss: 0.25845173
completed_steps 10300 - loss: 0.10226638
completed_steps 10350 - loss: 0.02627216
completed_steps 10400 - loss: 0.04825495
completed_steps 10450 - loss: 0.01162956
completed_steps 10500 - loss: 0.10183969
completed_steps 10550 - loss: 0.04218815
completed_steps 10600 - loss: 0.01445753
completed_steps 10650 - loss: 0.06203561
completed_steps 10700 - loss: 0.03677728
completed_steps 10750 - loss: 0.09402126
completed_steps 10800 - loss: 0.02360237
completed_steps 10850 - loss: 0.05790053
completed_steps 10900 - loss: 0.01970877
completed_steps 10950 - loss: 0.10294859
completed_steps 11000 - loss: 0.03649573
epoch 6: {'accuracy': 0.7528813559322034}
completed_steps 11050 - loss: 0.02209345
completed_steps 11100 - loss: 0.14103769
completed_steps 11150 - loss: 0.03566057
completed_steps 11200 - loss: 0.00622830
completed_steps 11250 - loss: 0.07047283
completed_steps 11300 - loss: 0.01187379
completed_steps 11350 - loss: 0.02625668
completed_steps 11400 - loss: 0.00291893
completed_steps 11450 - loss: 0.00071692
completed_steps 11500 - loss: 0.00287202
completed_steps 11550 - loss: 0.00741100
completed_steps 11600 - loss: 0.00880087
completed_steps 11650 - loss: 0.01264969
completed_steps 11700 - loss: 0.00768139
completed_steps 11750 - loss: 0.00162604
completed_steps 11800 - loss: 0.01595733
completed_steps 11850 - loss: 0.00372260
completed_steps 11900 - loss: 0.04441650
completed_steps 11950 - loss: 0.00944011
completed_steps 12000 - loss: 0.11529282
completed_steps 12050 - loss: 0.23609155
completed_steps 12100 - loss: 0.05506403
completed_steps 12150 - loss: 0.08533603
completed_steps 12200 - loss: 0.00222125
completed_steps 12250 - loss: 0.01137337
completed_steps 12300 - loss: 0.02558390
completed_steps 12350 - loss: 0.00256161
completed_steps 12400 - loss: 0.01695818
completed_steps 12450 - loss: 0.06429887
completed_steps 12500 - loss: 0.01197544
completed_steps 12550 - loss: 0.01158763
completed_steps 12600 - loss: 0.06567645
epoch 7: {'accuracy': 0.7457627118644068}
completed_steps 12650 - loss: 0.01072765
completed_steps 12700 - loss: 0.00257161
completed_steps 12750 - loss: 0.00083134
completed_steps 12800 - loss: 0.05551963
completed_steps 12850 - loss: 0.02928175
completed_steps 12900 - loss: 0.09139269
completed_steps 12950 - loss: 0.02321657
completed_steps 13000 - loss: 0.00843786
completed_steps 13050 - loss: 0.04562907
completed_steps 13100 - loss: 0.02042191
completed_steps 13150 - loss: 0.00461189
completed_steps 13200 - loss: 0.00200798
completed_steps 13250 - loss: 0.02195356
completed_steps 13300 - loss: 0.05659203
completed_steps 13350 - loss: 0.01498829
completed_steps 13400 - loss: 0.00997469
completed_steps 13450 - loss: 0.11865461
completed_steps 13500 - loss: 0.04770536
completed_steps 13550 - loss: 0.01439483
completed_steps 13600 - loss: 0.05105289
completed_steps 13650 - loss: 0.09264611
completed_steps 13700 - loss: 0.03426071
completed_steps 13750 - loss: 0.01838705
completed_steps 13800 - loss: 0.04093752
completed_steps 13850 - loss: 0.00752947
completed_steps 13900 - loss: 0.07661495
completed_steps 13950 - loss: 0.01379157
completed_steps 14000 - loss: 0.00364139
completed_steps 14050 - loss: 0.06416150
completed_steps 14100 - loss: 0.37660199
completed_steps 14150 - loss: 0.03790724
epoch 8: {'accuracy': 0.7505084745762712}
completed_steps 14200 - loss: 0.04396487
completed_steps 14250 - loss: 0.00169945
completed_steps 14300 - loss: 0.00721015
completed_steps 14350 - loss: 0.08254315
completed_steps 14400 - loss: 0.07593793
completed_steps 14450 - loss: 0.02416188
completed_steps 14500 - loss: 0.02093538
completed_steps 14550 - loss: 0.00464315
completed_steps 14600 - loss: 0.00611079
completed_steps 14650 - loss: 0.00373199
completed_steps 14700 - loss: 0.19110605
completed_steps 14750 - loss: 0.24078837
completed_steps 14800 - loss: 0.14356607
completed_steps 14850 - loss: 0.00222366
completed_steps 14900 - loss: 0.00070125
completed_steps 14950 - loss: 0.00037341
completed_steps 15000 - loss: 0.00043343
completed_steps 15050 - loss: 0.01910855
completed_steps 15100 - loss: 0.00132650
completed_steps 15150 - loss: 0.00789911
completed_steps 15200 - loss: 0.00075018
completed_steps 15250 - loss: 0.00501657
completed_steps 15300 - loss: 0.03107267
completed_steps 15350 - loss: 0.01066630
completed_steps 15400 - loss: 0.01643715
completed_steps 15450 - loss: 0.00065606
completed_steps 15500 - loss: 0.06498068
completed_steps 15550 - loss: 0.01089405
completed_steps 15600 - loss: 0.02065922
completed_steps 15650 - loss: 0.08452546
completed_steps 15700 - loss: 0.00295602
completed_steps 15750 - loss: 0.29694515
epoch 9: {'accuracy': 0.7450847457627119}
