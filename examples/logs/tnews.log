Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Mixed precision type: no

Sample 41905 of the training set: {'input_ids': [101, 2782, 2790, 340, 994, 2829, 6548, 306, 1278, 4176, 3472, 198, 4334, 792, 687, 2798, 309, 241, 7679, 2782, 2790, 340, 5261, 5891, 582, 2829, 6548, 306, 4334, 4176, 3472, 928, 7679, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 2}.
Sample 7296 of the training set: {'input_ids': [101, 7065, 270, 7210, 226, 7675, 2893, 2921, 2375, 2878, 4334, 2148, 6544, 585, 904, 6657, 1756, 484, 6987, 7679, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 4}.
Sample 1639 of the training set: {'input_ids': [101, 829, 2008, 2186, 4080, 5557, 6520, 232, 309, 241, 1776, 6994, 6175, 225, 3287, 3037, 175, 6213, 6250, 176, 451, 5083, 2736, 7679, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 11}.
***** Running training *****
  Num examples = 53360
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 16680
completed_steps 50 - loss: 1.98029029
completed_steps 100 - loss: 1.74407482
completed_steps 150 - loss: 1.78495252
completed_steps 200 - loss: 1.67423797
completed_steps 250 - loss: 1.39744139
completed_steps 300 - loss: 1.37697244
completed_steps 350 - loss: 1.23335826
completed_steps 400 - loss: 1.31528246
completed_steps 450 - loss: 0.81451613
completed_steps 500 - loss: 1.21195364
completed_steps 550 - loss: 1.08573842
completed_steps 600 - loss: 1.08062780
completed_steps 650 - loss: 1.49168777
completed_steps 700 - loss: 1.58733296
completed_steps 750 - loss: 1.05200362
completed_steps 800 - loss: 1.30316615
completed_steps 850 - loss: 1.28984690
completed_steps 900 - loss: 1.18185115
completed_steps 950 - loss: 1.36128569
completed_steps 1000 - loss: 0.94290197
completed_steps 1050 - loss: 1.43817818
completed_steps 1100 - loss: 1.12152743
completed_steps 1150 - loss: 1.18383920
completed_steps 1200 - loss: 1.03190660
completed_steps 1250 - loss: 1.31316817
completed_steps 1300 - loss: 1.26279616
completed_steps 1350 - loss: 1.16069567
completed_steps 1400 - loss: 0.95533198
completed_steps 1450 - loss: 1.48274338
completed_steps 1500 - loss: 1.44989789
completed_steps 1550 - loss: 1.45544493
completed_steps 1600 - loss: 0.76524758
completed_steps 1650 - loss: 0.94919741
epoch 0: {'accuracy': 0.5675}
completed_steps 1700 - loss: 1.13066852
completed_steps 1750 - loss: 1.14899457
completed_steps 1800 - loss: 1.42045951
completed_steps 1850 - loss: 1.12787497
completed_steps 1900 - loss: 1.02264094
completed_steps 1950 - loss: 1.03080189
completed_steps 2000 - loss: 1.07137883
completed_steps 2050 - loss: 0.76609981
completed_steps 2100 - loss: 1.17598867
completed_steps 2150 - loss: 0.95431727
completed_steps 2200 - loss: 1.07184815
completed_steps 2250 - loss: 1.24074841
completed_steps 2300 - loss: 1.24540997
completed_steps 2350 - loss: 1.30321193
completed_steps 2400 - loss: 1.25349462
completed_steps 2450 - loss: 0.61329287
completed_steps 2500 - loss: 1.44883597
completed_steps 2550 - loss: 0.56258953
completed_steps 2600 - loss: 0.63294780
completed_steps 2650 - loss: 1.06628823
completed_steps 2700 - loss: 0.84683657
completed_steps 2750 - loss: 0.81860334
completed_steps 2800 - loss: 1.33682013
completed_steps 2850 - loss: 0.82829463
completed_steps 2900 - loss: 1.16168857
completed_steps 2950 - loss: 0.98162204
completed_steps 3000 - loss: 1.19529986
completed_steps 3050 - loss: 1.01173055
completed_steps 3100 - loss: 1.06140864
completed_steps 3150 - loss: 1.04493773
completed_steps 3200 - loss: 1.04765832
completed_steps 3250 - loss: 0.92399561
completed_steps 3300 - loss: 1.17378533
epoch 1: {'accuracy': 0.5717}
completed_steps 3350 - loss: 0.92520881
completed_steps 3400 - loss: 0.63068241
completed_steps 3450 - loss: 0.56634086
completed_steps 3500 - loss: 0.69615436
completed_steps 3550 - loss: 0.70202893
completed_steps 3600 - loss: 0.90380794
completed_steps 3650 - loss: 1.14346087
completed_steps 3700 - loss: 0.65089452
completed_steps 3750 - loss: 0.83703589
completed_steps 3800 - loss: 0.67797852
completed_steps 3850 - loss: 0.94015467
completed_steps 3900 - loss: 0.78813809
completed_steps 3950 - loss: 0.52735472
completed_steps 4000 - loss: 0.97689366
completed_steps 4050 - loss: 0.55738974
completed_steps 4100 - loss: 0.94405025
completed_steps 4150 - loss: 0.73314857
completed_steps 4200 - loss: 0.62644762
completed_steps 4250 - loss: 0.71893638
completed_steps 4300 - loss: 0.43039060
completed_steps 4350 - loss: 0.63360530
completed_steps 4400 - loss: 0.52524251
completed_steps 4450 - loss: 0.79893178
completed_steps 4500 - loss: 0.62314165
completed_steps 4550 - loss: 0.78509760
completed_steps 4600 - loss: 0.63587332
completed_steps 4650 - loss: 1.24798512
completed_steps 4700 - loss: 0.66846198
completed_steps 4750 - loss: 0.81837696
completed_steps 4800 - loss: 0.71276921
completed_steps 4850 - loss: 0.87538278
completed_steps 4900 - loss: 0.88737756
completed_steps 4950 - loss: 0.58165556
completed_steps 5000 - loss: 0.86907947
epoch 2: {'accuracy': 0.5752}
completed_steps 5050 - loss: 0.45637488
completed_steps 5100 - loss: 0.34996083
completed_steps 5150 - loss: 0.78068519
completed_steps 5200 - loss: 0.36944899
completed_steps 5250 - loss: 0.47190738
completed_steps 5300 - loss: 0.34706762
completed_steps 5350 - loss: 0.50011313
completed_steps 5400 - loss: 0.77879083
completed_steps 5450 - loss: 0.53906369
completed_steps 5500 - loss: 0.75815451
completed_steps 5550 - loss: 0.78586650
completed_steps 5600 - loss: 0.72134095
completed_steps 5650 - loss: 0.27733210
completed_steps 5700 - loss: 0.40780365
completed_steps 5750 - loss: 0.56088632
completed_steps 5800 - loss: 0.62800574
completed_steps 5850 - loss: 0.45891103
completed_steps 5900 - loss: 0.65594375
completed_steps 5950 - loss: 0.66369075
completed_steps 6000 - loss: 0.56127620
completed_steps 6050 - loss: 0.70638990
completed_steps 6100 - loss: 0.38887709
completed_steps 6150 - loss: 0.75799692
completed_steps 6200 - loss: 0.51750845
completed_steps 6250 - loss: 0.74232733
completed_steps 6300 - loss: 0.49367562
completed_steps 6350 - loss: 0.47992811
completed_steps 6400 - loss: 0.69193661
completed_steps 6450 - loss: 0.57337499
completed_steps 6500 - loss: 0.71580797
completed_steps 6550 - loss: 0.30808955
completed_steps 6600 - loss: 0.55287194
completed_steps 6650 - loss: 0.45310712
epoch 3: {'accuracy': 0.5729}
completed_steps 6700 - loss: 0.23812418
completed_steps 6750 - loss: 0.30654964
completed_steps 6800 - loss: 0.49775368
completed_steps 6850 - loss: 0.26811135
completed_steps 6900 - loss: 0.27132490
completed_steps 6950 - loss: 0.40864113
completed_steps 7000 - loss: 0.18596357
completed_steps 7050 - loss: 0.28925279
completed_steps 7100 - loss: 0.23914523
completed_steps 7150 - loss: 0.28567898
completed_steps 7200 - loss: 0.20948340
completed_steps 7250 - loss: 0.37242734
completed_steps 7300 - loss: 0.37189955
completed_steps 7350 - loss: 0.30862296
completed_steps 7400 - loss: 0.24518059
completed_steps 7450 - loss: 0.37099978
completed_steps 7500 - loss: 0.36444160
completed_steps 7550 - loss: 0.28752932
completed_steps 7600 - loss: 0.27493307
completed_steps 7650 - loss: 0.17971192
completed_steps 7700 - loss: 0.31491747
completed_steps 7750 - loss: 0.46473026
completed_steps 7800 - loss: 0.20419085
completed_steps 7850 - loss: 0.38877052
completed_steps 7900 - loss: 0.45964339
completed_steps 7950 - loss: 0.38105747
completed_steps 8000 - loss: 0.22226034
completed_steps 8050 - loss: 0.34332398
completed_steps 8100 - loss: 0.49116084
completed_steps 8150 - loss: 0.31431696
completed_steps 8200 - loss: 0.24412209
completed_steps 8250 - loss: 0.35033241
completed_steps 8300 - loss: 0.22182664
epoch 4: {'accuracy': 0.5693}
completed_steps 8350 - loss: 0.27066597
completed_steps 8400 - loss: 0.20706937
completed_steps 8450 - loss: 0.16840875
completed_steps 8500 - loss: 0.31601351
completed_steps 8550 - loss: 0.20387028
completed_steps 8600 - loss: 0.21198656
completed_steps 8650 - loss: 0.12297104
completed_steps 8700 - loss: 0.12590669
completed_steps 8750 - loss: 0.27600327
completed_steps 8800 - loss: 0.14524761
completed_steps 8850 - loss: 0.16018482
completed_steps 8900 - loss: 0.13060476
completed_steps 8950 - loss: 0.12119301
completed_steps 9000 - loss: 0.31830025
completed_steps 9050 - loss: 0.19099094
completed_steps 9100 - loss: 0.06488977
completed_steps 9150 - loss: 0.48213553
completed_steps 9200 - loss: 0.13768773
completed_steps 9250 - loss: 0.48577511
completed_steps 9300 - loss: 0.04963570
completed_steps 9350 - loss: 0.25038230
completed_steps 9400 - loss: 0.45799094
completed_steps 9450 - loss: 0.18731458
completed_steps 9500 - loss: 0.16283557
completed_steps 9550 - loss: 0.29758272
completed_steps 9600 - loss: 0.35730439
completed_steps 9650 - loss: 0.27608791
completed_steps 9700 - loss: 0.78407866
completed_steps 9750 - loss: 0.32400179
completed_steps 9800 - loss: 0.24624562
completed_steps 9850 - loss: 0.31567612
completed_steps 9900 - loss: 0.36220586
completed_steps 9950 - loss: 0.14482255
completed_steps 10000 - loss: 0.17022526
epoch 5: {'accuracy': 0.5726}
completed_steps 10050 - loss: 0.06109130
completed_steps 10100 - loss: 0.13925567
completed_steps 10150 - loss: 0.14707690
completed_steps 10200 - loss: 0.25748295
completed_steps 10250 - loss: 0.09867913
completed_steps 10300 - loss: 0.17143370
completed_steps 10350 - loss: 0.19045869
completed_steps 10400 - loss: 0.23611398
completed_steps 10450 - loss: 0.22843497
completed_steps 10500 - loss: 0.06185857
completed_steps 10550 - loss: 0.07309349
completed_steps 10600 - loss: 0.09704532
completed_steps 10650 - loss: 0.07376245
completed_steps 10700 - loss: 0.09823953
completed_steps 10750 - loss: 0.07389626
completed_steps 10800 - loss: 0.32092983
completed_steps 10850 - loss: 0.11763214
completed_steps 10900 - loss: 0.14237337
completed_steps 10950 - loss: 0.05376897
completed_steps 11000 - loss: 0.26611108
completed_steps 11050 - loss: 0.17230445
completed_steps 11100 - loss: 0.06169493
completed_steps 11150 - loss: 0.03316451
completed_steps 11200 - loss: 0.26908973
completed_steps 11250 - loss: 0.19216551
completed_steps 11300 - loss: 0.32832947
completed_steps 11350 - loss: 0.26188949
completed_steps 11400 - loss: 0.09788733
completed_steps 11450 - loss: 0.09823536
completed_steps 11500 - loss: 0.10905071
completed_steps 11550 - loss: 0.07301787
completed_steps 11600 - loss: 0.13829981
completed_steps 11650 - loss: 0.15990968
epoch 6: {'accuracy': 0.5749}
completed_steps 11700 - loss: 0.07726454
completed_steps 11750 - loss: 0.05222751
completed_steps 11800 - loss: 0.15495679
completed_steps 11850 - loss: 0.20714693
completed_steps 11900 - loss: 0.27063769
completed_steps 11950 - loss: 0.09134328
completed_steps 12000 - loss: 0.07641442
completed_steps 12050 - loss: 0.29819888
completed_steps 12100 - loss: 0.43923202
completed_steps 12150 - loss: 0.18201588
completed_steps 12200 - loss: 0.07288422
completed_steps 12250 - loss: 0.07038958
completed_steps 12300 - loss: 0.06484943
completed_steps 12350 - loss: 0.04604412
completed_steps 12400 - loss: 0.16956329
completed_steps 12450 - loss: 0.11061126
completed_steps 12500 - loss: 0.11442481
completed_steps 12550 - loss: 0.11216580
completed_steps 12600 - loss: 0.18593387
completed_steps 12650 - loss: 0.08518436
completed_steps 12700 - loss: 0.16163376
completed_steps 12750 - loss: 0.60652143
completed_steps 12800 - loss: 0.11303773
completed_steps 12850 - loss: 0.18421423
completed_steps 12900 - loss: 0.02613105
completed_steps 12950 - loss: 0.17953244
completed_steps 13000 - loss: 0.31072390
completed_steps 13050 - loss: 0.24179821
completed_steps 13100 - loss: 0.45370388
completed_steps 13150 - loss: 0.03364772
completed_steps 13200 - loss: 0.18508412
completed_steps 13250 - loss: 0.56008756
completed_steps 13300 - loss: 0.32960346
epoch 7: {'accuracy': 0.5736}
completed_steps 13350 - loss: 0.04558788
completed_steps 13400 - loss: 0.06522718
completed_steps 13450 - loss: 0.02212416
completed_steps 13500 - loss: 0.28359058
completed_steps 13550 - loss: 0.15283372
completed_steps 13600 - loss: 0.07920320
completed_steps 13650 - loss: 0.01902297
completed_steps 13700 - loss: 0.08732095
completed_steps 13750 - loss: 0.14516583
completed_steps 13800 - loss: 0.01916869
completed_steps 13850 - loss: 0.01206324
completed_steps 13900 - loss: 0.04668591
completed_steps 13950 - loss: 0.05634760
completed_steps 14000 - loss: 0.12019940
completed_steps 14050 - loss: 0.03268489
completed_steps 14100 - loss: 0.13448283
completed_steps 14150 - loss: 0.03273893
completed_steps 14200 - loss: 0.10465469
completed_steps 14250 - loss: 0.01909956
completed_steps 14300 - loss: 0.04150580
completed_steps 14350 - loss: 0.05152199
completed_steps 14400 - loss: 0.06948560
completed_steps 14450 - loss: 0.02723233
completed_steps 14500 - loss: 0.07477597
completed_steps 14550 - loss: 0.14441530
completed_steps 14600 - loss: 0.14202167
completed_steps 14650 - loss: 0.13911502
completed_steps 14700 - loss: 0.16178311
completed_steps 14750 - loss: 0.01899190
completed_steps 14800 - loss: 0.05683694
completed_steps 14850 - loss: 0.21327417
completed_steps 14900 - loss: 0.11872257
completed_steps 14950 - loss: 0.09007664
completed_steps 15000 - loss: 0.21276490
epoch 8: {'accuracy': 0.5741}
completed_steps 15050 - loss: 0.03275356
completed_steps 15100 - loss: 0.23494130
completed_steps 15150 - loss: 0.10990766
completed_steps 15200 - loss: 0.01493944
completed_steps 15250 - loss: 0.19560073
completed_steps 15300 - loss: 0.10216596
completed_steps 15350 - loss: 0.05256065
completed_steps 15400 - loss: 0.01733667
completed_steps 15450 - loss: 0.01562915
completed_steps 15500 - loss: 0.03965014
completed_steps 15550 - loss: 0.03520643
completed_steps 15600 - loss: 0.11579216
completed_steps 15650 - loss: 0.15512364
completed_steps 15700 - loss: 0.04220396
completed_steps 15750 - loss: 0.11188424
completed_steps 15800 - loss: 0.08411897
completed_steps 15850 - loss: 0.01992240
completed_steps 15900 - loss: 0.04592845
completed_steps 15950 - loss: 0.08385944
completed_steps 16000 - loss: 0.10721797
completed_steps 16050 - loss: 0.03727261
completed_steps 16100 - loss: 0.14653368
completed_steps 16150 - loss: 0.06014299
completed_steps 16200 - loss: 0.31078434
completed_steps 16250 - loss: 0.02418257
completed_steps 16300 - loss: 0.00967813
completed_steps 16350 - loss: 0.08449884
completed_steps 16400 - loss: 0.10733861
completed_steps 16450 - loss: 0.15376842
completed_steps 16500 - loss: 0.04057504
completed_steps 16550 - loss: 0.08120871
completed_steps 16600 - loss: 0.01264824
completed_steps 16650 - loss: 0.17393398
epoch 9: {'accuracy': 0.5716}
