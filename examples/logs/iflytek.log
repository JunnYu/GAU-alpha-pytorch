Distributed environment: NO
Num processes: 1
Process index: 0
Local process index: 0
Device: cuda
Mixed precision type: no

Sample 10476 of the training set: {'input_ids': [101, 1299, 2467, 583, 4078, 3509, 6741, 6302, 274, 3910, 2905, 7675, 6020, 4369, 6331, 4078, 178, 4824, 4078, 178, 5340, 5931, 6429, 178, 6302, 6429, 178, 4189, 4720, 178, 2600, 660, 178, 7396, 1763, 1468, 178, 5079, 4078, 178, 3199, 3144, 4078, 178, 3086, 4078, 178, 6347, 3244, 4762, 7709, 4635, 6501, 735, 7209, 4377, 7675, 583, 1976, 6325, 6498, 8480, 7209, 6302, 274, 7675, 6566, 9890, 1288, 3289, 6302, 7396, 3574, 6030, 7233, 4381, 2654, 179, 226, 1727, 868, 2769, 4334, 394, 5218, 601, 1717, 6030, 7233, 178, 4381, 2654, 178, 1265, 2714, 178, 6735, 6104, 178, 5340, 693, 5402, 4377, 4762, 583, 1608, 394, 2079, 2047, 7675, 232, 2234, 2578, 434, 1699, 2769, 394, 5218, 6283, 6090, 994, 2737, 7000, 1475, 2920, 7675, 6020, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 46}.
Sample 1824 of the training set: {'input_ids': [101, 4652, 735, 163, 903, 4843, 2798, 4186, 225, 1264, 4652, 735, 6268, 1914, 585, 904, 2037, 873, 4334, 192, 3233, 6772, 1746, 579, 2689, 2901, 590, 994, 350, 274, 212, 810, 389, 4334, 2382, 2901, 1997, 4179, 179, 294, 1028, 336, 810, 389, 601, 6649, 291, 3476, 232, 3023, 2125, 7675, 2709, 913, 7106, 1248, 163, 5079, 178, 6548, 6090, 2073, 178, 3514, 2208, 178, 601, 6649, 2737, 7000, 4762, 192, 4896, 677, 730, 5261, 7675, 2878, 736, 277, 810, 389, 601, 6649, 911, 4935, 2350, 968, 4334, 2757, 1960, 291, 3476, 179, 917, 2769, 7675, 294, 1028, 260, 2798, 225, 1264, 4652, 735, 6268, 1914, 585, 904, 4958, 198, 2881, 732, 4334, 4700, 885, 179, 1508, 2878, 591, 6330, 7675, 896, 5183, 4896, 225, 1264, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 113}.
Sample 409 of the training set: {'input_ids': [101, 908, 6738, 1036, 1102, 206, 3429, 7688, 1816, 563, 4724, 5466, 6146, 943, 5643, 7675, 320, 940, 178, 6153, 178, 6160, 178, 612, 1242, 223, 5008, 2008, 7675, 668, 4935, 668, 7658, 7675, 336, 1036, 1036, 5102, 6146, 6162, 232, 3023, 2125, 394, 4896, 6162, 4657, 7675, 6084, 1669, 1650, 4896, 4989, 2179, 394, 4896, 780, 6509, 5931, 5466, 6146, 1668, 261, 179, 6772, 1746, 7681, 1816, 1698, 1698, 4334, 563, 3238, 2761, 2699, 6162, 4657, 4635, 1650, 6362, 4958, 2878, 2691, 4334, 4556, 5167, 2900, 2384, 5261, 1956, 736, 1669, 1650, 1375, 597, 659, 1506, 4334, 6146, 2289, 179, 4635, 1650, 6362, 4958, 6548, 6498, 7694, 7314, 4973, 595, 4334, 563, 3238, 1756, 4878, 940, 2699, 1668, 3418, 957, 4045, 4983, 2737, 2382, 3902, 1513, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'labels': 71}.
***** Running training *****
  Num examples = 12133
  Num Epochs = 10
  Instantaneous batch size per device = 32
  Total train batch size (w. parallel, distributed & accumulation) = 32
  Gradient Accumulation steps = 1
  Total optimization steps = 3800
completed_steps 50 - loss: 3.32615495
completed_steps 100 - loss: 2.76439285
completed_steps 150 - loss: 2.52490258
completed_steps 200 - loss: 2.17405605
completed_steps 250 - loss: 1.87476563
completed_steps 300 - loss: 1.98690820
completed_steps 350 - loss: 2.00965595
epoch 0: {'accuracy': 0.5744517121969989}
completed_steps 400 - loss: 1.20460677
completed_steps 450 - loss: 1.06159687
completed_steps 500 - loss: 1.51860356
completed_steps 550 - loss: 1.56827557
completed_steps 600 - loss: 1.61661053
completed_steps 650 - loss: 1.61272681
completed_steps 700 - loss: 1.16398692
completed_steps 750 - loss: 1.27114892
epoch 1: {'accuracy': 0.61177375913813}
completed_steps 800 - loss: 1.11531854
completed_steps 850 - loss: 1.52135253
completed_steps 900 - loss: 1.61117232
completed_steps 950 - loss: 1.27656710
completed_steps 1000 - loss: 1.48482525
completed_steps 1050 - loss: 1.12470794
completed_steps 1100 - loss: 1.57575202
epoch 2: {'accuracy': 0.6102347056560216}
completed_steps 1150 - loss: 0.85547960
completed_steps 1200 - loss: 0.99253923
completed_steps 1250 - loss: 1.69494486
completed_steps 1300 - loss: 0.69530147
completed_steps 1350 - loss: 0.82283902
completed_steps 1400 - loss: 0.87495124
completed_steps 1450 - loss: 0.89817828
completed_steps 1500 - loss: 1.27811968
epoch 3: {'accuracy': 0.599461331281262}
completed_steps 1550 - loss: 0.93808967
completed_steps 1600 - loss: 0.84926230
completed_steps 1650 - loss: 0.90987974
completed_steps 1700 - loss: 0.85136837
completed_steps 1750 - loss: 0.74102330
completed_steps 1800 - loss: 0.52354670
completed_steps 1850 - loss: 1.45734346
completed_steps 1900 - loss: 0.72701657
epoch 4: {'accuracy': 0.6060023085802232}
completed_steps 1950 - loss: 0.51646906
completed_steps 2000 - loss: 0.67170006
completed_steps 2050 - loss: 0.53078330
completed_steps 2100 - loss: 0.44006062
completed_steps 2150 - loss: 0.56594038
completed_steps 2200 - loss: 1.03964043
completed_steps 2250 - loss: 0.60479069
epoch 5: {'accuracy': 0.6040784917275875}
completed_steps 2300 - loss: 0.62066752
completed_steps 2350 - loss: 0.54234642
completed_steps 2400 - loss: 0.41063538
completed_steps 2450 - loss: 0.39994806
completed_steps 2500 - loss: 0.70773184
completed_steps 2550 - loss: 0.60621560
completed_steps 2600 - loss: 0.32186353
completed_steps 2650 - loss: 0.23674066
epoch 6: {'accuracy': 0.6010003847633705}
completed_steps 2700 - loss: 0.29187563
completed_steps 2750 - loss: 0.47496590
completed_steps 2800 - loss: 0.53090304
completed_steps 2850 - loss: 0.33163843
completed_steps 2900 - loss: 0.47312295
completed_steps 2950 - loss: 0.29238084
completed_steps 3000 - loss: 0.48213142
epoch 7: {'accuracy': 0.5983070411696807}
completed_steps 3050 - loss: 0.26820990
completed_steps 3100 - loss: 0.21629977
completed_steps 3150 - loss: 0.22213870
completed_steps 3200 - loss: 0.26453468
completed_steps 3250 - loss: 0.29218799
completed_steps 3300 - loss: 0.35187808
completed_steps 3350 - loss: 0.23777692
completed_steps 3400 - loss: 0.78406584
epoch 8: {'accuracy': 0.5852250865717584}
completed_steps 3450 - loss: 0.28150976
completed_steps 3500 - loss: 0.17730752
completed_steps 3550 - loss: 0.11162527
completed_steps 3600 - loss: 0.23177700
completed_steps 3650 - loss: 0.19964005
completed_steps 3700 - loss: 0.19408321
completed_steps 3750 - loss: 0.45455083
completed_steps 3800 - loss: 0.03473255
epoch 9: {'accuracy': 0.5782993459022701}
